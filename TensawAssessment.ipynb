{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3XyszXLfr0f4u+TU0JnEK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec8OijUrr7YQ"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index faiss-cpu redis fastapi uvicorn sentence-transformers openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index faiss-cpu redis fastapi uvicorn sentence-transformers openai\n",
        "\n",
        "import os\n",
        "import faiss\n",
        "import redis\n",
        "import fastapi\n",
        "import uvicorn\n",
        "from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, StorageContext, load_index_from_storage, QueryEngine\n",
        "from llama_index.query_engine import RetrieverQueryEngine\n",
        "from llama_index.llms import OpenAI\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from llama_index.embeddings import HuggingFaceEmbedding\n",
        "from fastapi import FastAPI, HTTPException\n",
        "import json\n",
        "from pydantic import BaseModel\n",
        "from typing import List"
      ],
      "metadata": {
        "id": "TnqelbiqtsY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set API Keys"
      ],
      "metadata": {
        "id": "vYVkolrcuB9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPEN_API_KEY\"] = \"Your api key will be here\""
      ],
      "metadata": {
        "id": "Tq7oo31suAM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Basic LlamaIndex Setup\n",
        "● Task: Install and set up LlamaIndex in a new Python project.\n",
        "● Use Case: A company wants to explore RAG-based AI chatbots, and the first step is setting up the basic LlamaIndex library"
      ],
      "metadata": {
        "id": "AWDUqmcTueUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = SimpleDirectoryReader('/content/sample_data').load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n"
      ],
      "metadata": {
        "id": "DvAvsVnMuZZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Constructing a Simple Index\n",
        "● Task: Write a Python script that:\n",
        "○ Loads a small text document (e.g., data.txt).\n",
        "○ Creates a LlamaIndex VectorStoreIndex.\n",
        "○ Saves the index and reloads it.\n",
        "● Use Case: A law firm wants to build an internal document retrieval system where employees can search for legal references quickly"
      ],
      "metadata": {
        "id": "eeCJVdHNvYr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "index.storage_context.persist(persist_dir=\"/content/sample_data/data.txt\")\n",
        "# Reload the index\n",
        "storage_context = StorageContext.from_defaults(persist_dir=\"/content/sample_data/data.txt\")\n",
        "index = load_index_from_storage(storage_context)\n"
      ],
      "metadata": {
        "id": "0nETNiL9vYH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Implementing a Basic RAG Pipeline\n",
        "● Task: Create a simple RAG pipeline using:\n",
        "○ OpenAI GPT as the LLM.\n",
        "○ FAISS as the vector store.\n",
        "○ LlamaIndex for retrieval."
      ],
      "metadata": {
        "id": "HxW-PSnhxUHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a FAISS vector store\n",
        "vector_store = FaissVectorStore(faiss_index=faiss.IndexFlatL2(1536))\n",
        "\n",
        "# Create a service context with the FAISS vector store and OpenAI LLM\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "service_context = ServiceContext.from_defaults(llm=OpenAI(temperature=0, model=\"gpt-3.5-turbo\"), embed_model=embed_model, vector_store=vector_store)\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "response = query_engine.query(\"What is in the documents?\")\n",
        "response\n"
      ],
      "metadata": {
        "id": "I43UOk-rxe2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Customizing the Indexing Process\n",
        "● Task: Modify the default LlamaIndex indexing process to:\n",
        "○ Split documents into smaller chunks (e.g., 256 tokens).\n",
        "○ Use a different embedding model (e.g., sentence-transformers/all-MiniLM-L6-v2).\n",
        "● Use Case: A tech company needs to break down its API documentation into smaller, searchable segments to help developers find relevant code snippets."
      ],
      "metadata": {
        "id": "wjhEMukByKd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.text_splitter import TokenTextSplitter\n",
        "\n",
        "# Define chunk size and embedding model\n",
        "chunk_size = 256\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Create a text splitter\n",
        "text_splitter = TokenTextSplitter(chunk_size=chunk_size)\n",
        "# Create a service context with the new settings\n",
        "service_context = ServiceContext.from_defaults(llm=OpenAI(temperature=0, model=\"gpt-3.5-turbo\"), embed_model=embed_model, text_splitter=text_splitter)\n",
        "\n",
        "# Create the index with the custom service context\n",
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What is in the documents?\")\n",
        "response\n"
      ],
      "metadata": {
        "id": "1nonDtI8x7Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Query Engine Customization\n",
        "● Task: Implement a custom retriever that ranks retrieved documents based on keyword matching before passing them to the LLM.\n",
        "● Use Case: A university wants to create a chatbot that helps students find course materials, prioritizing documents that contain course codes and professor names."
      ],
      "metadata": {
        "id": "8ROT31Z2yV5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.retrievers import BaseRetriever\n",
        "from typing import List\n",
        "from llama_index.schema import NodeWithScore\n",
        "\n",
        "class KeywordMatchingRetriever(BaseRetriever):\n",
        "    def __init__(self, index):\n",
        "        self._index = index\n",
        "\n",
        "    def _retrieve(self, query_str: str) -> List[NodeWithScore]:\n",
        "        # Simple keyword matching (replace with more sophisticated logic)\n",
        "        keywords = query_str.lower().split()\n",
        "        scored_nodes = []\n",
        "        for node in self._index.docstore.docs.values():\n",
        "            score = 0\n",
        "            for keyword in keywords:\n",
        "                if keyword in node.text.lower():\n",
        "                    score += 1\n",
        "            scored_nodes.append(NodeWithScore(node, score))\n",
        "\n",
        "        # Sort by score\n",
        "        scored_nodes.sort(key=lambda x: x.score, reverse=True)\n",
        "        return scored_nodes\n",
        "\n",
        "retriever = KeywordMatchingRetriever(index)\n",
        "query_engine = RetrieverQueryEngine.from_args(retriever, service_context=service_context)\n",
        "\n",
        "response = query_engine.query(\"What is in the documents?\")\n",
        "response\n"
      ],
      "metadata": {
        "id": "soc6hzwOybPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. LlamaIndex Query Transformation\n",
        "● Task: Implement a query transformer that:\n",
        "○ Expands the user’s query using synonyms before sending it to the index.\n",
        "● Use Case: A medical information system needs to ensure that if a doctor searches for “cardiac issues,” results for “heart disease” and “cardiovascular conditions” are also retrieved."
      ],
      "metadata": {
        "id": "yRUw98pczw6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.indices.query.query_transform.base import QueryTransform\n",
        "from typing import List\n",
        "\n",
        "class SynonymQueryTransformer(QueryTransform):\n",
        "    def __init__(self, synonym_map):\n",
        "        self._synonym_map = synonym_map\n",
        "    def _transform(self, query_str: str) -> str:\n",
        "        expanded_query = [query_str]\n",
        "        for word in query_str.split():\n",
        "            if word in self._synonym_map:\n",
        "                expanded_query.extend(self._synonym_map[word])\n",
        "        return \" OR \".join(expanded_query)\n",
        "synonym_map = {\n",
        "    \"cardiac issues\": [\"heart disease\", \"cardiovascular conditions\"],\n",
        "    # Add more synonyms as needed\n",
        "}\n",
        "query_transformer = SynonymQueryTransformer(synonym_map)\n",
        "# Example query\n",
        "query_str = \"cardiac issues\"\n",
        "transformed_query = query_transformer.transform(query_str)\n",
        "print(f\"Original query: {query_str}\")\n",
        "print(f\"Transformed query: {transformed_query}\")\n",
        "# Integrate with your query engine:\n",
        "query_engine = index.as_query_engine(query_transform=query_transformer)\n",
        "esponse = query_engine.query(\"cardiac issues\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "PidrR4qfz13U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Hybrid Search (Vector + Keyword)\n",
        "● Task: Modify LlamaIndex to use a hybrid search strategy that combines:\n",
        "○ Vector similarity search (e.g., FAISS).\n",
        "○ BM25 keyword search (e.g., using llama-index.query_engine.RetrieverQueryEngine).\n",
        "● Use Case: A news aggregator wants users to find articles using semantic similarity and exact keyword matches, ensuring better coverage of trending topics."
      ],
      "metadata": {
        "id": "-UxyW4Mq0Uml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.retrievers import BM25Retriever\n",
        "from llama_index.query_engine import RetrieverQueryEngine\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_defaults(index)\n",
        "# Combine retrievers\n",
        "def hybrid_retrieve(query_str):\n",
        "    vector_results = index.as_retriever().retrieve(query_str)\n",
        "    bm25_results = bm25_retriever.retrieve(query_str)\n",
        "    # Combine the results\n",
        "    combined_results = {}\n",
        "    for node_with_score in vector_results:\n",
        "        combined_results[node_with_score.node.node_id] = node_with_score.score\n",
        "    for node_with_score in bm25_results:\n",
        "        if node_with_score.node.node_id in combined_results:\n",
        "            combined_results[node_with_score.node.node_id] += node_with_score.score\n",
        "        else:\n",
        "            combined_results[node_with_score.node.node_id] = node_with_score.score\n",
        "    sorted_results = sorted(combined_results.items(), key=lambda x: x[1], reverse=True)\n",
        "    return [NodeWithScore(index.docstore.get_node(node_id), score) for node_id, score in sorted_results]\n",
        "# Create a hybrid query engine\n",
        "hybrid_query_engine = RetrieverQueryEngine.from_args(hybrid_retrieve, service_context=service_context)\n",
        "# Test the hybrid query engine\n",
        "response = hybrid_query_engine.query(\"What is in the documents?\")\n",
        "response\n"
      ],
      "metadata": {
        "id": "_eL-GZgK0bOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Caching with LlamaIndex\n",
        "● Task: Implement query caching using Redis or a local file-based approach to store previously queried results.\n",
        "● Use Case: A customer service AI needs to improve response time by storing frequently asked questions so that repetitive queries do not trigger expensive LLM calls."
      ],
      "metadata": {
        "id": "EWiK41ru0yav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.indices.query.query_engine import QueryEngine\n",
        "from llama_index.indices.vector_store.base import VectorStoreIndex\n",
        "from llama_index import StorageContext, load_index_from_storage\n",
        "import json\n",
        "\n",
        "class CacheQueryEngine(QueryEngine):\n",
        "    def __init__(self, query_engine: QueryEngine, cache_dir: str):\n",
        "        self._query_engine = query_engine\n",
        "        self._cache_dir = cache_dir\n",
        "        os.makedirs(self._cache_dir, exist_ok=True)\n",
        "\n",
        "    def query(self, query_str: str):\n",
        "        cache_file = os.path.join(self._cache_dir, f\"{hash(query_str)}.json\")\n",
        "\n",
        "        if os.path.exists(cache_file):\n",
        "            with open(cache_file, \"r\") as f:\n",
        "                cached_response = json.load(f)\n",
        "                print(\"Returning from cache\")\n",
        "                return cached_response\n",
        "        else:\n",
        "            response = self._query_engine.query(query_str)\n",
        "            with open(cache_file, \"w\") as f:\n",
        "                json.dump(response.response, f, indent=4)\n",
        "            print(\"Storing in cache\")\n",
        "            return response\n",
        "storage_context = StorageContext.from_defaults(persist_dir=\"/content/sample_data/data.txt\")\n",
        "index = load_index_from_storage(storage_context)\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "# Create the cache query engine\n",
        "cached_query_engine = CacheQueryEngine(query_engine, \"/content/cache\")\n",
        "\n",
        "# Test the cached query engine\n",
        "response1 = cached_query_engine.query(\"What is in the documents?\")\n",
        "print(response1)\n",
        "\n",
        "response2 = cached_query_engine.query(\"What is in the documents?\")\n",
        "response2\n"
      ],
      "metadata": {
        "id": "uV615Va50x3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. API Integration with FastAPI\n",
        "● Task: Create a REST API with FastAPI that:\n",
        "○ Accepts a query via a /query endpoint.\n",
        "○ Uses a LlamaIndex-powered RAG system to fetch and generate responses.\n",
        "● Use Case: A business intelligence tool needs to allow users to search company reports via an API, returning relevant insights dynamically."
      ],
      "metadata": {
        "id": "cHY0SOnr1RoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "class QueryRequest(BaseModel):\n",
        "    query: str\n",
        "\n",
        "@app.post(\"/query\")\n",
        "async def query_endpoint(request: QueryRequest):\n",
        "    query_str = request.query\n",
        "    try:\n",
        "        response = cached_query_engine.query(query_str) # Use your existing query engine\n",
        "        return {\"response\": response.response}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Error processing query: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ],
      "metadata": {
        "id": "NFz_m0zU1YUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Fine-tuning the RAG Model\n",
        "● Task: Fine-tune the retrieval process by:\n",
        "○ Adjusting similarity thresholds.\n",
        "○ Implementing a re-ranking model (e.g., Cohere’s reranker or BERT-based re-ranking).\n",
        "● Use Case: An e-commerce company wants a product recommendation AI that retrieves the most relevant products based on user queries, improving search relevance"
      ],
      "metadata": {
        "id": "yCsRrXcN1iiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.indices.query.query_transform.base import QueryTransform\n",
        "from llama_index.retrievers import BaseRetriever\n",
        "from llama_index.schema import NodeWithScore\n",
        "from typing import List\n",
        "\n",
        "# Assuming 'index' and 'service_context' are defined from previous code blocks\n",
        "\n",
        "class RerankingRetriever(BaseRetriever):\n",
        "    def __init__(self, index, similarity_threshold=0.7):\n",
        "        self._index = index\n",
        "        self.similarity_threshold = similarity_threshold\n",
        "\n",
        "    def _retrieve(self, query_str: str) -> List[NodeWithScore]:\n",
        "        vector_results = self._index.as_retriever().retrieve(query_str)\n",
        "\n",
        "        # 2. Re-ranking based on similarity score and threshold\n",
        "        reranked_results = []\n",
        "        for node_with_score in vector_results:\n",
        "          if node_with_score.score >= self.similarity_threshold:\n",
        "            reranked_results.append(node_with_score)\n",
        "        # Sort by score again after filtering\n",
        "        reranked_results.sort(key=lambda x: x.score, reverse=True)\n",
        "        return reranked_results\n",
        "\n",
        "# Example usage:\n",
        "reranking_retriever = RerankingRetriever(index, similarity_threshold=0.8)\n",
        "query_engine = RetrieverQueryEngine.from_args(reranking_retriever, service_context=service_context)\n",
        "response = query_engine.query(\"What is in the documents?\")\n",
        "response\n"
      ],
      "metadata": {
        "id": "i_uaXb7j1z5E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}